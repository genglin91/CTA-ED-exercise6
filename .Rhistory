knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
url <- "https://www.ssa.gov/oact/babynames/numberUSbirths.html"
# Storing the url's HTML code
html_content <- read_html(url)
# Not very informative
str(html_content)
print(html_content)
html_content
# Looking at first 1000 characters
substr(html_content, 1, 1000)
# Extracting tables in the document
tab <- html_table(html_content, fill = TRUE)
# Check object
str(tab)
tab
# Save the dataframe (tibble)
social_security_data <- tab[[1]]
social_security_data <- as_tibble(social_security_data)
social_security_data
tab
tab <- html_table(html_content, fill = TRUE)
# Check object
str(tab)
# Website only had one table -> list of length 1 containing the dataframe
# Save the dataframe (tibble)
social_security_data <- tab[[1]]
social_security_data <- as_tibble(social_security_data)
social_security_data
social_security_data
tab
## Remove commas and then make variable numeric
social_security_data$Male <-  as.numeric(gsub(",", "", social_security_data$Male))
social_security_data$Female <- as.numeric(gsub(",", "", social_security_data$Female))
social_security_data$Total <- as.numeric(gsub(",", "", social_security_data$Total))
## Rename variables
colnames(social_security_data) <- c('year', 'male', 'female', 'total')
social_security_data
# Pivot longer function
social_security_data_long <- pivot_longer(data = social_security_data[, 1:3],
cols = c("male", "female"), names_to = "sex", values_to = "individuals")
# Creating the plot
ggplot(social_security_data_long) +
aes(x = year, y = individuals, group = sex, colour = sex) +
geom_line() + xlab("Year") + ylab("Individuals") +
scale_colour_manual(name ="Birth Sex", values = c(1, 2), labels = c("Female", "Male"))
url <- "https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population"
html <- read_html(url)
tables <- html_table(html, fill=TRUE)
class(tables)
length(tables)
print(tables)
tables[3] # position 3 happens to be the correct table (checked 12/03/2024)
table_raw <- html_elements(html, css = 'tba')
length(table_raw) # now a list of length 1
data_pop <- html_table(table_raw[[1]], fill = TRUE)
tables[3] # position 3 happens to be the correct table (checked 12/03/2024)
table_raw <- html_elements(html, css = 'tba')
length(table_raw) # now a list of length 1
data_pop <- html_table(table_raw[[1]], fill = TRUE)
social_security_data_long
Sys.getenv('NAME')
library("httr")
library("jsonlite")
library("tidyverse")
library("tidyverse")
library("jpeg") #to let us read .jpegs/.jpgs
library("grid") #to let us plot images
artworks_url <- "https://api.artic.edu/api/v1/artworks"
fromJSON(artworks_url)
fromJSON(artworks_url)
artists_url <- "https://api.artic.edu/api/v1/artists"
fromJSON(artists_url)
artworks_url_fields <- "https://api.artic.edu/api/v1/artworks?fields=id,title,artist_display,date_display"
fromJSON(artworks_url_fields)
# define our fields of interest
fields <- "?fields=id,title,artist_display,date_display"
# provide an artwork to study
artwork <- "28560"
# build the query and retrieve JSON
artwork_detail_url <- paste0(artworks_url, "/", artwork, fields)
fromJSON(artwork_detail_url)
fromJSON(artwork_detail_url)
# to show only the data we want
fromJSON(artwork_detail_url)$data
# artworks model, search endpoint url:
artworks_search_url <- "https://api.artic.edu/api/v1/artworks/search?q="
# define search terms. we use gsub(" ", "%20", "x") here to replace spaces between search terms with "%20" which is how we often represent spaces in a URL.
search_terms <- gsub(" ", "%20", "cat")
# build the query:
cat_search_url <- paste0(artworks_search_url, search_terms)
fromJSON(cat_search_url)
# build the API GET request
cat_search <- GET(artworks_search_url, # the API endpoint of interest
query = list(q = search_terms,
fields = "id,title,artist_display,date_display",
size = 10)) # query allows us to specify parameters, which we find in the API documentation
# build the API GET request
cat_search <- GET(artworks_search_url, # the API endpoint of interest
query = list(q = search_terms,
fields = "id,title,artist_display,date_display",
size = 10)) # query allows us to specify parameters, which we find in the API documentation
# parse the content returned from our GET request
json_cat_search <- content(cat_search, "parsed")
# let's inspect our content
json_cat_search
# not so useful! so let's see what we got in a slightly easier way...
names(json_cat_search)
# $data is what we want. so let's use do.call, rbind, and lapply to extract all the data from our returned content, and format it as a tidy tibble
cat_art <- do.call(rbind, lapply(json_cat_search$data, as_tibble, stringsAsFactors = FALSE)) %>%
select(- '_score') # removing the search score, but you can keep it if interesting to you
# let's look at our tibble
cat_art
# query the API:
cat_image_search <- GET(artworks_search_url, # the API endpoint of interest
query = list(q = search_terms,
fields = "title, image_id",
size = 1)) # query allows us to specify parameters, which we find in the API documentation
# query the API:
cat_image_search <- GET(artworks_search_url, # the API endpoint of interest
query = list(q = search_terms,
fields = "title, image_id",
size = 1)) # query allows us to specify parameters, which we find in the API documentation
json_cat_image_search <- content(cat_image_search, "parsed")
# directly extract the image id (as we are just working with one request, we don't need to worry about flattening the data)
cat_image_id <- json_cat_image_search$data[[1]]$image_id
# now, we introduce our alternative API, the AIC's IIIF (International Image Interoperability Framework) API
iiif_url <- "https://www.artic.edu/iiif/2"
# using our iiif_url and our cat_image_id, plus some formatting as provided by the AIC API documentation, we get
iiif_url_artwork <- paste0(iiif_url, "/", cat_image_id, "/full/843,/0/default.jpg")
# assign an empty temporary file to store our downloaded image in this R session (in a moment we will save these locally, when we do a retrieve of images)
temp <- tempfile()
# download the file from our API URL
download.file(iiif_url_artwork, temp, mode="wb")
class(image_to_plot)
# plot our image, using ggplot (can also use base R)
ggplot() +
annotation_custom(rasterGrob(image_to_plot), xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf) +
theme_void() +
theme(plot.margin = unit(rep(0, 4), "null"))
#Reading the file from the temp object
image_to_plot <- readJPEG(temp)
class(image_to_plot)
# plot our image, using ggplot (can also use base R)
ggplot() +
annotation_custom(rasterGrob(image_to_plot), xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf) +
theme_void() +
theme(plot.margin = unit(rep(0, 4), "null"))
# let's build a function
art_image_search <- function(search_term, n_images = 5, output_dir = "temp_images", clear_directory = TRUE, plot_images = TRUE) {
search_term <- gsub(" ", "%20", search_term)
images_search_url <- "https://api.artic.edu/api/v1/artworks/search?q="
images_search_out <- GET(images_search_url, # the API endpoint of interest
query = list(q = search_term,
fields = "id, title, artist_display, image_id",
size = n_images)) # query allows us to specify parameters, which we find in the API documentation
json_images_search_out <- content(images_search_out, "parsed")
# replace NULL values with NA values
json_images_search_out$data <- eval(parse(text = gsub("NULL", "NA", deparse(json_images_search_out$data))))
image_ids <- do.call(rbind, lapply(json_images_search_out$data, as_tibble, stringsAsFactors = FALSE)) %>%
dplyr::select('id', 'title', 'artist_display', 'image_id')
# we now check if our output directory exists. if not, we create it. if it does and we want to clear the directory, we do so. else, proceed.
if (!dir.exists(paste0("./",output_dir))) {
dir.create(paste0("./",output_dir))
} else if(dir.exists(paste0("./",output_dir)) & clear_directory == TRUE) {
unlink(paste0("./",output_dir), recursive = TRUE, force = TRUE)
dir.create(paste0("./",output_dir))
} else {}
# now move to image API query
iiif_url <- "https://www.artic.edu/iiif/2"
# now work through the image ids, with api queries:
for(i in 1:nrow(image_ids)){
file <- paste0("./", output_dir, "/", image_ids$id[i], ".jpg")
# try() here allows our request to fail without interrupting the run
try(download.file(paste0(iiif_url, "/", image_ids$image_id[i], "/full/843,/0/default.jpg"),
file, mode="wb"))
# take a breath
Sys.sleep(1)
}
# enumerate our successfully downloaded files
downloads <- list.files(paste0("./", output_dir))
# now, if we want to plot images, we save them to a list of ggplots
if (plot_images == TRUE){
images <- list()
for(j in 1:length(downloads)){
image_to_plot <- readJPEG(paste0("./", output_dir,"/", downloads[j]))
id <- gsub(".jpg", "", downloads[j])
artist <- image_ids$artist_display[image_ids$id==id]
title <- image_ids$title[image_ids$id==id]
title_for_image <- paste0(title, " by ", artist)
images[[j]] <- ggplot() +
annotation_custom(rasterGrob(image_to_plot), xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf) +
ggtitle(str_wrap(title_for_image, 80)) +
theme_void()
}
# return the object
return(images)
} else {}
}
modern_art_images <- art_image_search("modern art", 10)
modern_art_images
